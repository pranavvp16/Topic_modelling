{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12fbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\",shuffle=True)\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\",shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173929cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238ff6f",
   "metadata": {},
   "source": [
    "## Now Data preprocessing to feed to LDA model\n",
    "The data will be be processed in the following steps\n",
    "- Tokenization : Split the text into sentence and sentences into words,also all words are lowered and punctuation is removed\n",
    "- All stopwords are removed, i.e helping verbs(mostly)\n",
    "- Words are lemmatized : words in third person are changed to first person and words in past and future tenses are changed to present\n",
    "- Words are stemmed : words are converted to there base class like dancing is converted to danc which is root of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb89270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/pranavprajapati/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/pranavprajapati/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/pranavprajapati/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/pranavprajapati/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27df1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f58eb2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tried', 'so', 'hard']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_preprocess(\"I tried so hard \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b739d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bf2a5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/pranavprajapati/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f391ae7",
   "metadata": {},
   "source": [
    "### Lemmatizer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3dff940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('ran',pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7483c7",
   "metadata": {},
   "source": [
    "### Stemmer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "317eb920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orginal_words</th>\n",
       "      <th>base_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dancing</td>\n",
       "      <td>danc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loving</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horses</td>\n",
       "      <td>hors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beautifully</td>\n",
       "      <td>beauti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orginal_words base_words\n",
       "0       dancing       danc\n",
       "1        loving       love\n",
       "2        horses       hors\n",
       "3           ran        ran\n",
       "4   sensational     sensat\n",
       "5   beautifully     beauti\n",
       "6         flies        fli\n",
       "7          dies        die"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stemmer = SnowballStemmer('english')\n",
    "original_words = [\"dancing\", \"loving\" , \"horses\",\"ran\", \"sensational\", \"beautifully\", \"flies\", \"dies\"]\n",
    "base_words = [stemmer.stem(words) for words in original_words]\n",
    "pd.DataFrame(data={\"orginal_words\":original_words,\"base_words\":base_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ca82061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function to preprocess data in the entire dataset\n",
    "\"\"\"\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocessor(text):\n",
    "    result =[]\n",
    "    for token in simple_preprocess(text):\n",
    "        if token is not STOPWORDS and len(token)>3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba38d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'tried', 'so', 'hard', 'and', 'got', 'so', 'far,', 'in', 'the', 'end', 'it', \"doesn't\", 'even', 'matter']\n",
      "['tri', 'hard', 'doesn', 'even', 'matter']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Test the function before pre-processing on the data \n",
    "\"\"\"\n",
    "sent_sample = \"I tried so hard and got so far, in the end it doesn't even matter\"\n",
    "words = []\n",
    "for word in sent_sample.split(' '):\n",
    "    words.append(word)\n",
    "    \n",
    "print(words)\n",
    "#Tokenizing and running the preprocessing function\n",
    "print(preprocessor(sent_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bad85d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now preprocess you main data using the function\n",
    "prepro_data = []\n",
    "for doc in newsgroups_train.data:\n",
    "    prepro_data.append(preprocessor(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b8b85",
   "metadata": {},
   "source": [
    "## Create a bag of words with the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53948e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Create a dictionary from preprocessed data containing the number of times word appears in the training dataset \n",
    "\"\"\"\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(prepro_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db328171",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.1,keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af1994d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Create a bag of words for each document \n",
    "\"\"\"\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in prepro_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb69ec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 15 (\"name\") appears 1 time.\n",
      "Word 19 (\"rest\") appears 1 time.\n",
      "Word 116 (\"rather\") appears 1 time.\n",
      "Word 173 (\"clear\") appears 1 time.\n",
      "Word 352 (\"refer\") appears 1 time.\n",
      "Word 366 (\"true\") appears 1 time.\n",
      "Word 380 (\"abov\") appears 1 time.\n",
      "Word 410 (\"technolog\") appears 1 time.\n",
      "Word 463 (\"christian\") appears 1 time.\n",
      "Word 479 (\"exampl\") appears 1 time.\n",
      "Word 502 (\"jew\") appears 1 time.\n",
      "Word 506 (\"lead\") appears 1 time.\n",
      "Word 508 (\"littl\") appears 3 time.\n",
      "Word 549 (\"wors\") appears 2 time.\n",
      "Word 756 (\"keith\") appears 3 time.\n",
      "Word 766 (\"punish\") appears 1 time.\n",
      "Word 843 (\"california\") appears 1 time.\n",
      "Word 905 (\"institut\") appears 1 time.\n",
      "Word 964 (\"similar\") appears 1 time.\n",
      "Word 1037 (\"allan\") appears 1 time.\n",
      "Word 1038 (\"anti\") appears 1 time.\n",
      "Word 1039 (\"arriv\") appears 1 time.\n",
      "Word 1040 (\"austria\") appears 1 time.\n",
      "Word 1041 (\"caltech\") appears 2 time.\n",
      "Word 1042 (\"distinguish\") appears 1 time.\n",
      "Word 1043 (\"german\") appears 1 time.\n",
      "Word 1044 (\"germani\") appears 3 time.\n",
      "Word 1045 (\"hitler\") appears 1 time.\n",
      "Word 1046 (\"least\") appears 1 time.\n",
      "Word 1047 (\"livesey\") appears 2 time.\n",
      "Word 1048 (\"motto\") appears 2 time.\n",
      "Word 1049 (\"order\") appears 1 time.\n",
      "Word 1050 (\"pasadena\") appears 1 time.\n",
      "Word 1051 (\"pompous\") appears 1 time.\n",
      "Word 1052 (\"popul\") appears 1 time.\n",
      "Word 1053 (\"rank\") appears 1 time.\n",
      "Word 1054 (\"schneider\") appears 1 time.\n",
      "Word 1055 (\"semit\") appears 1 time.\n",
      "Word 1056 (\"social\") appears 1 time.\n",
      "Word 1057 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28690142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c007501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(bow_corpus,num_topics=8,\n",
    "                        id2word=dictionary,\n",
    "                        passes=10,\n",
    "                        workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01cb48ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.009*\"govern\" + 0.006*\"encrypt\" + 0.005*\"israel\" + 0.005*\"public\" + 0.005*\"secur\" + 0.004*\"isra\" + 0.004*\"chip\" + 0.004*\"presid\" + 0.004*\"clipper\" + 0.004*\"american\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.014*\"armenian\" + 0.008*\"turkish\" + 0.007*\"kill\" + 0.005*\"live\" + 0.004*\"greek\" + 0.004*\"turk\" + 0.004*\"down\" + 0.004*\"armenia\" + 0.004*\"turkey\" + 0.004*\"leav\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.016*\"window\" + 0.012*\"file\" + 0.008*\"program\" + 0.007*\"card\" + 0.006*\"version\" + 0.006*\"softwar\" + 0.005*\"imag\" + 0.005*\"graphic\" + 0.005*\"avail\" + 0.005*\"color\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.017*\"space\" + 0.014*\"drive\" + 0.013*\"nasa\" + 0.009*\"scsi\" + 0.007*\"control\" + 0.007*\"data\" + 0.006*\"orbit\" + 0.006*\"disk\" + 0.005*\"launch\" + 0.004*\"hard\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.006*\"wire\" + 0.004*\"power\" + 0.004*\"food\" + 0.004*\"caus\" + 0.004*\"pitt\" + 0.004*\"ohio\" + 0.004*\"medic\" + 0.004*\"effect\" + 0.003*\"water\" + 0.003*\"health\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.010*\"christian\" + 0.007*\"jesus\" + 0.006*\"exist\" + 0.004*\"moral\" + 0.004*\"bibl\" + 0.004*\"word\" + 0.004*\"religion\" + 0.004*\"life\" + 0.004*\"church\" + 0.004*\"claim\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.008*\"drive\" + 0.007*\"bike\" + 0.007*\"price\" + 0.007*\"sale\" + 0.005*\"entri\" + 0.005*\"engin\" + 0.005*\"sell\" + 0.004*\"car\" + 0.004*\"speed\" + 0.004*\"buy\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.017*\"game\" + 0.014*\"team\" + 0.011*\"play\" + 0.009*\"player\" + 0.007*\"hockey\" + 0.006*\"season\" + 0.005*\"leagu\" + 0.005*\"score\" + 0.004*\"basebal\" + 0.003*\"divis\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417561bd",
   "metadata": {},
   "source": [
    "## Check the model on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7237ff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: help\n",
      "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
      "Lines: 13\n",
      "\n",
      "Hello All!\n",
      "\n",
      "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
      "prior to starting Windows - this makes getting into Windows quite slow if you\n",
      "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
      "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
      "maybe make something like a PIF file to load them only when you enter the\n",
      "applications that need fonts?  Any ideas?\n",
      "\n",
      "\n",
      "Chris\n",
      "\n",
      " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec116bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9717214107513428\t Topic: 0.016*\"window\" + 0.012*\"file\" + 0.008*\"program\" + 0.007*\"card\" + 0.006*\"version\"\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocessor(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a60c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
